{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qb4745/evaluacion_1_deep_learning/blob/main/evaluacion_1_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1. Introducción\n",
        "\n",
        "Este proyecto aborda un problema práctico de clasificación de imágenes utilizando una Red Neuronal Artificial (RNA) de tipo Perceptrón Multicapa (MLP). El objetivo es aplicar los fundamentos de Deep Learning para entrenar un modelo capaz de identificar diferentes tipos de artículos de moda a partir de imágenes en escala de grises.\n",
        "\n",
        "Para este propósito, utilizaremos el dataset **Fashion-MNIST**. Este conjunto de datos fue desarrollado por Zalando Research y se presenta como un reemplazo directo (\"drop-in replacement\") del clásico dataset MNIST de dígitos escritos a mano. Mientras que MNIST es un estándar ampliamente utilizado para la validación inicial de algoritmos de Machine Learning, Fashion-MNIST ofrece un desafío potencialmente mayor manteniendo la misma estructura y formato: un conjunto de entrenamiento de 60,000 imágenes y un conjunto de prueba de 10,000 imágenes, todas ellas en escala de grises y de tamaño 28x28 píxeles, distribuidas en 10 clases distintas.\n",
        "\n",
        "La elección de Fashion-MNIST nos permite trabajar con un dataset estandarizado, bien conocido en la comunidad, pero que requiere que el modelo aprenda características más complejas que las presentes en los dígitos manuscritos. El objetivo final de este encargo es implementar, entrenar, optimizar y evaluar un MLP utilizando TensorFlow/Keras para clasificar correctamente las imágenes de artículos de moda de este dataset, cumpliendo con los indicadores de logro especificados en la evaluación.\n",
        "\n",
        "*Fuente oficial del dataset: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)*\n",
        "\n",
        "## 2. Descripción del Dataset (Contenido)\n",
        "\n",
        "El dataset Fashion-MNIST se compone de imágenes y sus correspondientes etiquetas de clase. A continuación, se detallan sus características principales:\n",
        "\n",
        "*   **Dimensiones de la Imagen:** Cada imagen tiene una altura de 28 píxeles y un ancho de 28 píxeles, resultando en un total de 784 píxeles por imagen.\n",
        "*   **Formato de Píxeles:** Las imágenes están en escala de grises. Cada píxel tiene asociado un único valor numérico entero que indica su nivel de oscuridad. Este valor varía entre 0 (blanco) y 255 (negro). Valores más altos representan píxeles más oscuros.\n",
        "*   **Estructura de los Datos:** Cada Fila del Dataset representa una imagen:\n",
        "    *   **Primera Columna:** Corresponde a la etiqueta de clase (un número del 0 al 9) que identifica el tipo de artículo de moda representado en la imagen.\n",
        "    *   **Columnas Restantes (784):** Contienen los valores de los píxeles (0-255) de la imagen asociada, generalmente desplegados en un formato aplanado (vector de 784 elementos).\n",
        "*   **Etiquetas de Clase:** Cada ejemplo (imagen) está asignado a una de las siguientes 10 clases, representadas por un número entero:\n",
        "\n",
        "    *   `0`: T-shirt/top (Camiseta/Top)\n",
        "    *   `1`: Trouser (Pantalón)\n",
        "    *   `2`: Pullover (Suéter)\n",
        "    *   `3`: Dress (Vestido)\n",
        "    *   `4`: Coat (Abrigo)\n",
        "    *   `5`: Sandal (Sandalia)\n",
        "    *   `6`: Shirt (Camisa)\n",
        "    *   `7`: Sneaker (Zapatilla deportiva)\n",
        "    *   `8`: Bag (Bolso)\n",
        "    *   `9`: Ankle boot (Botín)\n",
        "\n",
        "**En resumen:** Cada fila del dataset representa una imagen de 28x28 píxeles en escala de grises, junto con una etiqueta numérica que indica a cuál de las 10 categorías de ropa pertenece. El objetivo del modelo será aprender a predecir esta etiqueta basándose en los 784 valores de píxeles de entrada.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "DZJqnXR5tqtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 3. Configuración del Entorno y Selección del Framework\n",
        "\n",
        "Antes de proceder con la carga y preprocesamiento de los datos, es fundamental definir el entorno de trabajo y las herramientas principales que se utilizarán para la implementación de la Red Neuronal Artificial (MLP).\n",
        "\n",
        "### 3.1. Elección del Framework: TensorFlow con Keras\n",
        "\n",
        "Para el desarrollo de este proyecto, hemos seleccionado **TensorFlow (versión X.Y.Z)** como la biblioteca principal de Deep Learning, utilizando específicamente su interfaz de alto nivel **Keras**.\n",
        "\n",
        "**Justificación de la Decisión:**\n",
        "\n",
        "La elección de TensorFlow con Keras se basa en las siguientes consideraciones clave, alineadas con los objetivos de la evaluación y los requisitos del proyecto:\n",
        "\n",
        "1.  **Facilidad de Uso y Desarrollo Rápido (API de Keras):** Keras proporciona una API intuitiva y modular que simplifica significativamente el proceso de definición, entrenamiento y evaluación de redes neuronales, incluyendo los MLP requeridos. Esto nos permite centrarnos en la aplicación de los conceptos fundamentales de Deep Learning (como la arquitectura del modelo, funciones de activación, optimizadores y métricas) en lugar de en detalles de implementación de bajo nivel.\n",
        "2.  **Cumplimiento de Requisitos:** TensorFlow/Keras ofrece todas las componentes necesarias para abordar los Indicadores de Logro de esta evaluación:\n",
        "    *   Capas densas para construir el MLP.\n",
        "    *   Amplia variedad de funciones de activación  y la flexibilidad para definir funciones personalizadas si fuera necesario .\n",
        "    *   Diversas funciones de pérdida adecuadas para problemas de clasificación.\n",
        "    *   Múltiples algoritmos de optimización para entrenar y optimizar el modelo.\n",
        "    *   Herramientas integradas para la evaluación del modelo y el cálculo de métricas.\n",
        "3.  **Documentación Extensa y Comunidad Activa:** TensorFlow y Keras cuentan con una documentación oficial muy completa, numerosos tutoriales y una vasta comunidad de usuarios. Esto facilita la resolución de dudas y la consulta de ejemplos durante el desarrollo.\n",
        "4.  **Integración con Ecosistema:** TensorFlow se integra fácilmente con otras herramientas útiles del ecosistema de Data Science en Python, como NumPy, Pandas y Scikit-learn (que utilizaremos para la carga y preprocesamiento de datos), así como con herramientas de visualización como Matplotlib/Seaborn y TensorBoard (para un análisis más profundo del entrenamiento, si se requiere).\n",
        "5.  **Entorno de Ejecución (Google Colab):** TensorFlow está preinstalado y optimizado para su uso en Google Colab, el entorno recomendado para este proyecto, permitiendo aprovechar fácilmente los recursos de hardware como GPUs o TPUs para acelerar el entrenamiento si fuera necesario.\n",
        "\n",
        "**En resumen**, la combinación TensorFlow/Keras representa un balance adecuado entre potencia, flexibilidad y facilidad de uso, lo que la convierte en una opción idónea para implementar eficientemente el MLP solicitado y cumplir con los objetivos de esta evaluación dentro del plazo establecido. Las siguientes secciones detallarán cómo se utiliza este framework para la carga, preprocesamiento, modelado y evaluación.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9keVEug9tuLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importación de Librerías\n",
        "\n",
        "En esta sección, importamos las bibliotecas fundamentales que utilizaremos a lo largo del proyecto: NumPy para operaciones numéricas y Keras (parte de TensorFlow) para construir y entrenar nuestro modelo de red neuronal."
      ],
      "metadata": {
        "id": "sT1sqWuGtyLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "yvRNMZwZt2C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparando los Datos"
      ],
      "metadata": {
        "id": "HSL5XHaPzdNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros del modelo / datos\n",
        "num_classes = 10 # Número de clases de salida (0-9)\n",
        "input_shape = (28, 28, 1) # Forma de entrada para cada imagen (28x28 píxeles, 1 canal de color gris)\n",
        "\n",
        "# Cargar los datos y dividirlos en conjuntos de entrenamiento y prueba\n",
        "# ¡Importante! Usamos fashion_mnist en lugar de mnist\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Escalar las imágenes al rango [0, 1]\n",
        "# Convertimos los valores de píxeles (originalmente 0-255) a float32 y los normalizamos\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# Asegurarse de que las imágenes tengan la forma (alto, ancho, canal)\n",
        "# Añadimos una dimensión extra para el canal (aunque sea 1 para escala de grises)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Imprimir información sobre las dimensiones de los datos cargados\n",
        "print(\"Forma de x_train:\", x_train.shape)\n",
        "print(x_train.shape[0], \"muestras de entrenamiento\")\n",
        "print(x_test.shape[0], \"muestras de prueba\")\n",
        "\n",
        "# Convertir los vectores de clase a matrices binarias de clase (one-hot encoding)\n",
        "# Esto es necesario para usar la función de pérdida 'categorical_crossentropy'\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(\"Forma de y_train (one-hot):\", y_train.shape)\n",
        "print(\"Ejemplo de y_train[0] (one-hot):\", y_train[0])"
      ],
      "metadata": {
        "id": "xKFt2szjzphK",
        "outputId": "e33563d7-dfcc-47a5-9e95-37b69e687ede",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Forma de x_train: (60000, 28, 28, 1)\n",
            "60000 muestras de entrenamiento\n",
            "10000 muestras de prueba\n",
            "Forma de y_train (one-hot): (60000, 10)\n",
            "Ejemplo de y_train[0] (one-hot): [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Definición del Modelo (Arquitectura CNN)\n",
        "En esta sección, definimos la arquitectura de nuestra red neuronal utilizando la API Sequential de Keras. Nota: Este modelo es una Red Neuronal Convolucional (CNN), que incluye capas especializadas para procesar datos espaciales como las imágenes."
      ],
      "metadata": {
        "id": "LkRhS4eR1pSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        # Define la capa de entrada con la forma esperada para cada imagen\n",
        "        keras.Input(shape=input_shape),\n",
        "        # Primera capa convolucional: extrae 32 patrones usando filtros 3x3. ReLU activa las neuronas.\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        # Capa de Max Pooling: reduce la dimensión espacial (a la mitad) para hacer el modelo más robusto y eficiente.\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        # Segunda capa convolucional: extrae 64 patrones más complejos de las características anteriores.\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        # Segunda capa de Max Pooling.\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        # Capa de Aplanamiento: convierte los mapas de características 2D resultantes en un vector 1D.\n",
        "        layers.Flatten(),\n",
        "        # Capa Dropout: técnica de regularización. Apaga aleatoriamente el 50% de las neuronas durante el entrenamiento para prevenir el sobreajuste.\n",
        "        layers.Dropout(0.5),\n",
        "        # Capa Densa de Salida: capa totalmente conectada con 10 neuronas (una por clase).\n",
        "        # La activación Softmax convierte las salidas en probabilidades para cada clase.\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ],\n",
        "    name=\"modelo_cnn_fashion_mnist\" # Es buena práctica darle un nombre al modelo\n",
        ")\n",
        "\n",
        "# Muestra un resumen de la arquitectura del modelo: capas, formas de salida y número de parámetros.\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "Mo-dLw4c1Vdt",
        "outputId": "e1ef4dd2-fb6e-453b-8cf4-c2fce5ac092a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"modelo_cnn_fashion_mnist\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"modelo_cnn_fashion_mnist\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m16,010\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,010</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,826\u001b[0m (136.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,826</span> (136.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,826\u001b[0m (136.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,826</span> (136.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Compilación del Modelo\n",
        "Antes de poder entrenar el modelo, necesitamos configurarlo mediante el método compile. Este paso define la función de pérdida que se minimizará, el optimizador que se usará para ajustar los pesos del modelo, y las métricas que queremos monitorizar durante el entrenamiento y la evaluación."
      ],
      "metadata": {
        "id": "HY95eqH-1zd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Compilación del modelo\n",
        "# -------------------------------\n",
        "# Definir el tamaño del lote y número de épocas\n",
        "batch_size = 128  # Número de muestras procesadas antes de actualizar los pesos\n",
        "epochs = 15       # Número de veces que el modelo verá el dataset completo\n",
        "\n",
        "# Compilar el modelo especificando la función de pérdida, el optimizador y las métricas\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",  # Función de pérdida para clasificación multiclase\n",
        "    optimizer=\"adam\",                 # Optimizador Adam\n",
        "    metrics=[\"accuracy\"]             # Métrica a monitorear durante el entrenamiento\n",
        ")\n",
        "\n",
        "print(\"Modelo compilado con éxito.\")"
      ],
      "metadata": {
        "id": "VNMBy4RO14MN",
        "outputId": "2a2b30de-c03f-4914-848f-163613ac7b58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo compilado con éxito.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Entrenamiento del Modelo\n",
        "Una vez compilado, el modelo está listo para ser entrenado con los datos de entrenamiento (x_train, y_train) utilizando el método fit.\n"
      ],
      "metadata": {
        "id": "rGrERnwu1jfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Entrenamiento del modelo\n",
        "# -------------------------------\n",
        "# Entrenar el modelo con los datos de entrenamiento\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.1  # Usar el 10% del set de entrenamiento para validación\n",
        ")"
      ],
      "metadata": {
        "id": "UK7GiHXA_uq_",
        "outputId": "e67fed89-6868-40e2-efa6-cc3c1d2e6b7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 95ms/step - accuracy: 0.9154 - loss: 0.2269 - val_accuracy: 0.9188 - val_loss: 0.2243\n",
            "Epoch 2/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 99ms/step - accuracy: 0.9190 - loss: 0.2176 - val_accuracy: 0.9203 - val_loss: 0.2257\n",
            "Epoch 3/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 99ms/step - accuracy: 0.9200 - loss: 0.2184 - val_accuracy: 0.9185 - val_loss: 0.2200\n",
            "Epoch 4/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 96ms/step - accuracy: 0.9236 - loss: 0.2092 - val_accuracy: 0.9175 - val_loss: 0.2222\n",
            "Epoch 5/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 95ms/step - accuracy: 0.9238 - loss: 0.2074 - val_accuracy: 0.9185 - val_loss: 0.2213\n",
            "Epoch 6/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 95ms/step - accuracy: 0.9263 - loss: 0.2069 - val_accuracy: 0.9197 - val_loss: 0.2231\n",
            "Epoch 7/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 98ms/step - accuracy: 0.9246 - loss: 0.2064 - val_accuracy: 0.9193 - val_loss: 0.2239\n",
            "Epoch 8/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 95ms/step - accuracy: 0.9251 - loss: 0.2040 - val_accuracy: 0.9175 - val_loss: 0.2221\n",
            "Epoch 9/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 102ms/step - accuracy: 0.9245 - loss: 0.2033 - val_accuracy: 0.9215 - val_loss: 0.2171\n",
            "Epoch 10/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 95ms/step - accuracy: 0.9245 - loss: 0.2040 - val_accuracy: 0.9202 - val_loss: 0.2243\n",
            "Epoch 11/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 97ms/step - accuracy: 0.9245 - loss: 0.2065 - val_accuracy: 0.9220 - val_loss: 0.2139\n",
            "Epoch 12/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 95ms/step - accuracy: 0.9229 - loss: 0.2038 - val_accuracy: 0.9185 - val_loss: 0.2226\n",
            "Epoch 13/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 97ms/step - accuracy: 0.9258 - loss: 0.2017 - val_accuracy: 0.9220 - val_loss: 0.2140\n",
            "Epoch 14/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 95ms/step - accuracy: 0.9271 - loss: 0.1988 - val_accuracy: 0.9253 - val_loss: 0.2155\n",
            "Epoch 15/15\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 96ms/step - accuracy: 0.9271 - loss: 0.2000 - val_accuracy: 0.9220 - val_loss: 0.2170\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7899b3d50990>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluación del Modelo en el Conjunto de Prueba\n",
        "Una vez que el modelo ha sido entrenado, el siguiente paso crucial es evaluar su rendimiento en el conjunto de prueba (x_test, y_test). Este conjunto contiene datos que el modelo no ha visto durante el proceso de entrenamiento ni durante la fase de validación (si se usó validation_split en fit). Esta evaluación nos proporciona una estimación imparcial de cómo se espera que el modelo generalice a nuevos datos desconocidos."
      ],
      "metadata": {
        "id": "WS-FhbP1AJKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el modelo entrenado utilizando el conjunto de prueba\n",
        "print(\"Evaluando el modelo en el conjunto de prueba...\")\n",
        "score = model.evaluate(x_test,  # Imágenes del conjunto de prueba\n",
        "                       y_test,  # Etiquetas verdaderas del conjunto de prueba (one-hot)\n",
        "                       verbose=0) # verbose=0 para modo silencioso (sin barra de progreso)\n",
        "\n",
        "# Imprimir las métricas resultantes: pérdida y exactitud (accuracy)\n",
        "# score[0] contiene el valor de la función de pérdida (loss)\n",
        "# score[1] contiene el valor de la métrica 'accuracy' (definida en compile)\n",
        "print(f\"Pérdida en el conjunto de prueba (Test loss): {score[0]:.4f}\")\n",
        "print(f\"Exactitud en el conjunto de prueba (Test accuracy): {score[1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "W8yJrPGMANYT",
        "outputId": "595dd7b2-08fa-4f81-9c4a-f78822480d1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluando el modelo en el conjunto de prueba...\n",
            "Pérdida en el conjunto de prueba (Test loss): 0.2407\n",
            "Exactitud en el conjunto de prueba (Test accuracy): 0.9128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# MLP para Fashion MNIST\n",
        "# ===============================\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "# -------------------------------\n",
        "# Parámetros globales\n",
        "# -------------------------------\n",
        "num_classes = 10         # Número de clases de salida (0-9)\n",
        "input_shape = (28, 28, 1)  # Forma de entrada para cada imagen\n",
        "batch_size = 128         # Tamaño de lote\n",
        "epochs = 15              # Número de épocas de entrenamiento\n",
        "learning_rate = 0.001    # Tasa de aprendizaje para el optimizador\n",
        "\n",
        "# -------------------------------\n",
        "# Carga y preprocesamiento de datos\n",
        "# -------------------------------\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Escalar imágenes a rango [0,1] y añadir dimensión de canal\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test  = x_test.astype(\"float32\")  / 255.0\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test  = np.expand_dims(x_test, -1)\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape}, {x_train.shape[0]} muestras\")\n",
        "print(f\"x_test  shape: {x_test.shape}, {x_test.shape[0]} muestras\")\n",
        "\n",
        "# One-hot encoding de etiquetas\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# -------------------------------\n",
        "# Definición del modelo MLP\n",
        "# -------------------------------\n",
        "model = keras.Sequential(name=\"mlp_fashion_mnist\")\n",
        "model.add(layers.Flatten(input_shape=input_shape))  # Aplanar entrada\n",
        "model.add(layers.Dense(128, activation=\"relu\", name=\"hidden_layer_1\"))\n",
        "model.add(layers.Dropout(0.5, name=\"dropout_1\"))\n",
        "model.add(layers.Dense(64, activation=\"relu\", name=\"hidden_layer_2\"))\n",
        "model.add(layers.Dropout(0.5, name=\"dropout_2\"))\n",
        "model.add(layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\"))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# -------------------------------\n",
        "# Compilación del modelo\n",
        "# -------------------------------\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "print(\"Modelo compilado con éxito.\")\n",
        "\n",
        "# -------------------------------\n",
        "# Entrenamiento del modelo\n",
        "# -------------------------------\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluación en el conjunto de prueba\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluando en conjunto de prueba:\")\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test loss: {score[0]:.4f}\")\n",
        "print(f\"Test accuracy: {score[1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "8m0cI4rqDWL4",
        "outputId": "d88dce99-c2a1-4de0-8d2b-9068568f1e7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (60000, 28, 28, 1), 60000 muestras\n",
            "x_test  shape: (10000, 28, 28, 1), 10000 muestras\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"mlp_fashion_mnist\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mlp_fashion_mnist\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hidden_layer_1 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hidden_layer_2 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hidden_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ hidden_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo compilado con éxito.\n",
            "Epoch 1/15\n",
            "422/422 - 62s - 147ms/step - accuracy: 0.6739 - loss: 0.9229 - val_accuracy: 0.8172 - val_loss: 0.4948\n",
            "Epoch 2/15\n",
            "422/422 - 2s - 5ms/step - accuracy: 0.7898 - loss: 0.6046 - val_accuracy: 0.8352 - val_loss: 0.4538\n",
            "Epoch 3/15\n",
            "422/422 - 2s - 5ms/step - accuracy: 0.8088 - loss: 0.5433 - val_accuracy: 0.8507 - val_loss: 0.4133\n",
            "Epoch 4/15\n",
            "422/422 - 3s - 6ms/step - accuracy: 0.8237 - loss: 0.5066 - val_accuracy: 0.8553 - val_loss: 0.3996\n",
            "Epoch 5/15\n",
            "422/422 - 3s - 8ms/step - accuracy: 0.8312 - loss: 0.4832 - val_accuracy: 0.8553 - val_loss: 0.3947\n",
            "Epoch 6/15\n",
            "422/422 - 2s - 5ms/step - accuracy: 0.8351 - loss: 0.4671 - val_accuracy: 0.8652 - val_loss: 0.3749\n",
            "Epoch 7/15\n",
            "422/422 - 2s - 6ms/step - accuracy: 0.8414 - loss: 0.4581 - val_accuracy: 0.8648 - val_loss: 0.3676\n",
            "Epoch 8/15\n",
            "422/422 - 3s - 6ms/step - accuracy: 0.8449 - loss: 0.4383 - val_accuracy: 0.8657 - val_loss: 0.3635\n",
            "Epoch 9/15\n",
            "422/422 - 2s - 5ms/step - accuracy: 0.8471 - loss: 0.4368 - val_accuracy: 0.8547 - val_loss: 0.3939\n",
            "Epoch 10/15\n",
            "422/422 - 3s - 7ms/step - accuracy: 0.8504 - loss: 0.4259 - val_accuracy: 0.8638 - val_loss: 0.3603\n",
            "Epoch 11/15\n",
            "422/422 - 2s - 6ms/step - accuracy: 0.8522 - loss: 0.4188 - val_accuracy: 0.8725 - val_loss: 0.3479\n",
            "Epoch 12/15\n",
            "422/422 - 2s - 5ms/step - accuracy: 0.8525 - loss: 0.4122 - val_accuracy: 0.8723 - val_loss: 0.3490\n",
            "Epoch 13/15\n",
            "422/422 - 3s - 6ms/step - accuracy: 0.8531 - loss: 0.4126 - val_accuracy: 0.8755 - val_loss: 0.3405\n",
            "Epoch 14/15\n",
            "422/422 - 2s - 5ms/step - accuracy: 0.8558 - loss: 0.4037 - val_accuracy: 0.8722 - val_loss: 0.3546\n",
            "Epoch 15/15\n",
            "422/422 - 3s - 7ms/step - accuracy: 0.8580 - loss: 0.3975 - val_accuracy: 0.8700 - val_loss: 0.3495\n",
            "\n",
            "Evaluando en conjunto de prueba:\n",
            "Test loss: 0.3717\n",
            "Test accuracy: 0.8669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entre los dos resultados:\n",
        "\n",
        "| Enfoque | Test Loss | Test Accuracy |\n",
        "|---------|-----------|---------------|\n",
        "| **CNN** | 0.2407    | **0.9128**    |\n",
        "| **MLP** | 0.3717    | 0.8669        |\n",
        "\n",
        "- **Exactitud (accuracy)**: la CNN alcanza un 91.28 %, frente al 86.69 % del MLP, es decir una ganancia de ≈ 4.6 puntos porcentuales.  \n",
        "- **Pérdida (loss)**: la CNN obtiene un valor más bajo (0.2407 vs 0.3717), lo que indica que sus predicciones están más cercanas a las distribuciones reales de las clases.\n",
        "\n",
        "**Conclusión:** la arquitectura **CNN** ofrece mejores métricas en clasificación de Fashion MNIST, gracias a su capacidad de explotar las estructuras espaciales de las imágenes (convoluciones y pooling) antes de llegar a la capa densa final. Si tu objetivo es maximizar precisión en datos de imagen, la CNN es la opción más adecuada."
      ],
      "metadata": {
        "id": "3jLyKG9HEP-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Mejorando el Modelo\n",
        "\n",
        "Para superar el ~91 % de accuracy en Fashion MNIST con una CNN básica, puedes probar varias de estas estrategias (idealmente combinándolas):  \n",
        "\n",
        "1. **Aumentar y diversificar los datos (Data Augmentation)**  \n",
        "   - Aplica transformaciones aleatorias durante el entrenamiento: rotaciones pequeñas, desplazamientos, zoom, flips horizontales.  \n",
        "   - Esto ayuda al modelo a generalizar mejor ante variaciones que no están en el set original.  \n",
        "\n",
        "   ```python\n",
        "   from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "   datagen = ImageDataGenerator(\n",
        "       rotation_range=10,\n",
        "       width_shift_range=0.1,\n",
        "       height_shift_range=0.1,\n",
        "       zoom_range=0.1,\n",
        "       horizontal_flip=True\n",
        "   )\n",
        "   datagen.fit(x_train)\n",
        "   # Luego, en lugar de model.fit, usas:\n",
        "   model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "             epochs=epochs,\n",
        "             validation_data=(x_test, y_test))\n",
        "   ```\n",
        "\n",
        "2. **Batch Normalization**  \n",
        "   - Inserta `layers.BatchNormalization()` tras cada Convolution + ReLU.  \n",
        "   - Estabiliza y acelera el entrenamiento, permitiendo tasas de aprendizaje más altas y mejor convergencia.\n",
        "\n",
        "3. **Arquitectura más profunda o ancha**  \n",
        "   - Añade una tercera o cuarta capa conv + pooling, aumentando progresivamente los filtros (por ejemplo, 128 filtros en la tercera).  \n",
        "   - Prueba también colocar una capa densa intermedia antes del softmax (por ejemplo 256 → 128 neuronas).\n",
        "\n",
        "4. **Regularización extra**  \n",
        "   - Ajusta el `Dropout` en cada bloque conv (ej. 0.25) y en las densas (ej. 0.5).  \n",
        "   - Considera añadir L2 weight decay en el optimizador:  \n",
        "     ```python\n",
        "     optimizer = keras.optimizers.Adam(learning_rate, decay=1e-4)\n",
        "     ```\n",
        "\n",
        "5. **Scheduler de tasa de aprendizaje**  \n",
        "   - Empieza con LR = 1e-3 y reduce en “plateau” (p.ej. factor 0.5 tras 3 épocas sin mejora).  \n",
        "   - O usa un ciclo de learning rate (CosineAnnealing, OneCycle).\n",
        "\n",
        "6. **Early Stopping y Checkpointing**  \n",
        "   - Monitorea la validación y guarda el mejor modelo para evitar overfitting.  \n",
        "   - Ejemplo:\n",
        "     ```python\n",
        "     callbacks = [\n",
        "       keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5),\n",
        "       keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "     ]\n",
        "     model.fit(..., callbacks=callbacks)\n",
        "     ```\n",
        "\n",
        "7. **Ensemble de modelos**  \n",
        "   - Entrena varias arquitecturas (o con diferentes inicializaciones) y promedia sus predicciones.  \n",
        "   - Suele dar un pequeño “boost” adicional en accuracy.\n",
        "\n",
        "8. **Transfer Learning ligero**  \n",
        "   - Aunque Fashion MNIST es muy diferente de ImageNet, puedes probar usar un bloque convolucional preentrenado (ej. MobileNetV2) y adaptar sus primeras capas, sustituyendo la parte final por densas custom.\n",
        "\n",
        "–––\n",
        "\n",
        "**Recomendación de orden de experimentos (controlados):**  \n",
        "\n",
        "1. Empieza por añadir **BatchNorm** y **ReduceLROnPlateau**.  \n",
        "2. Incorpora **data augmentation** y observa mejora de val_accuracy.  \n",
        "3. Prueba una capa conv extra o más filtros.  \n",
        "4. Ajusta dropout / L2 para equilibrar over/underfitting.  \n",
        "5. Si aún quieres exprimir más, combina dos o más modelos en **ensemble**.\n",
        "\n",
        "\n",
        "\n",
        "Data Augmentation para generar ejemplos variados.\n",
        "\n",
        "Batch Normalization para estabilizar el entrenamiento.\n",
        "\n",
        "Dropout más estratégico para evitar overfitting.\n",
        "\n",
        "Capas adicionales para aprender patrones más complejos.\n",
        "\n",
        "Callbacks: ReduceLROnPlateau y EarlyStopping para entrenar de forma más eficiente.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9u7WSXviF99G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Carga y preprocesamiento de datos\n",
        "# -------------------------------\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Data Augmentation\n",
        "# -------------------------------\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Modelo CNN mejorado\n",
        "# -------------------------------\n",
        "model = keras.Sequential([\n",
        "    keras.Input(shape=input_shape),\n",
        "\n",
        "    layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "], name=\"cnn_fashionmnist_improved\")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Compilación y entrenamiento con callbacks\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Evaluación final\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluando el modelo en el conjunto de prueba...\")\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Pérdida en el conjunto de prueba (Test loss): {score[0]:.4f}\")\n",
        "print(f\"Exactitud en el conjunto de prueba (Test accuracy): {score[1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "g8_aLqL5F7AG",
        "outputId": "93ae84fe-35db-4d3d-b82e-f24769a9f351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"cnn_fashionmnist_improved\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cnn_fashionmnist_improved\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1152\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m295,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1152</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m392,330\u001b[0m (1.50 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">392,330</span> (1.50 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m391,370\u001b[0m (1.49 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">391,370</span> (1.49 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m960\u001b[0m (3.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 346ms/step - accuracy: 0.6172 - loss: 1.1608 - val_accuracy: 0.6764 - val_loss: 0.8225 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 356ms/step - accuracy: 0.7757 - loss: 0.5961 - val_accuracy: 0.8412 - val_loss: 0.4320 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 349ms/step - accuracy: 0.8082 - loss: 0.5148 - val_accuracy: 0.8602 - val_loss: 0.3735 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 362ms/step - accuracy: 0.8268 - loss: 0.4597 - val_accuracy: 0.8566 - val_loss: 0.3833 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 350ms/step - accuracy: 0.8353 - loss: 0.4435 - val_accuracy: 0.8248 - val_loss: 0.5251 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 351ms/step - accuracy: 0.8434 - loss: 0.4217 - val_accuracy: 0.8828 - val_loss: 0.3156 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 358ms/step - accuracy: 0.8485 - loss: 0.4093 - val_accuracy: 0.8700 - val_loss: 0.3306 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 356ms/step - accuracy: 0.8550 - loss: 0.3903 - val_accuracy: 0.8125 - val_loss: 0.4856 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 353ms/step - accuracy: 0.8597 - loss: 0.3777 - val_accuracy: 0.8868 - val_loss: 0.2972 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 344ms/step - accuracy: 0.8627 - loss: 0.3683 - val_accuracy: 0.8872 - val_loss: 0.2976 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 345ms/step - accuracy: 0.8660 - loss: 0.3614 - val_accuracy: 0.8627 - val_loss: 0.3649 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 354ms/step - accuracy: 0.8650 - loss: 0.3569 - val_accuracy: 0.8926 - val_loss: 0.2865 - learning_rate: 0.0010\n",
            "Epoch 13/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 343ms/step - accuracy: 0.8685 - loss: 0.3526 - val_accuracy: 0.8813 - val_loss: 0.3057 - learning_rate: 0.0010\n",
            "Epoch 14/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 352ms/step - accuracy: 0.8698 - loss: 0.3546 - val_accuracy: 0.8502 - val_loss: 0.3855 - learning_rate: 0.0010\n",
            "Epoch 15/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 339ms/step - accuracy: 0.8722 - loss: 0.3408 - val_accuracy: 0.8758 - val_loss: 0.3322 - learning_rate: 0.0010\n",
            "Epoch 16/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 356ms/step - accuracy: 0.8804 - loss: 0.3226 - val_accuracy: 0.9032 - val_loss: 0.2616 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 391ms/step - accuracy: 0.8824 - loss: 0.3169 - val_accuracy: 0.9015 - val_loss: 0.2638 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m 52/469\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:16\u001b[0m 326ms/step - accuracy: 0.8942 - loss: 0.3011"
          ]
        }
      ]
    }
  ]
}